{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da9897b-4716-4eeb-b5f8-867159810d69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SaveHelper:\n",
    "    def __init__(self, df, layer, table_path, mode):\n",
    "        \"\"\"\n",
    "        Classe utilitária para fisicalizar dados.\n",
    "\n",
    "        Args:\n",
    "            df (spark.DataFrame): DataFrame com dados\n",
    "            layer (str): Camada para salvar\n",
    "            table_path (str): Caminho da tabela\n",
    "            mode (str): Modo de salvamento dos dados (append or overwrite)\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Se os parâmetros forem inválidos\n",
    "        \"\"\"\n",
    "        self._validate_inputs(df, layer, table_path, mode)\n",
    "\n",
    "        self.df = df\n",
    "        self.layer = layer.lower()\n",
    "        self.table_path = table_path\n",
    "        self.table_exists = spark.catalog.tableExists(self.table_path)\n",
    "        self.mode = mode.lower()\n",
    "\n",
    "        logger.info(f\"SaveHelper inicializado - Tabela: {table_path} - Mode: {mode} - Layer: {layer}\")\n",
    "\n",
    "    def _validate_inputs(self, df, layer, table_path, mode):\n",
    "        \"\"\"Valida os parâmetros de entrada\"\"\"\n",
    "        if df is None:\n",
    "            raise ValueError(\"DataFrame não pode ser None\")\n",
    "        \n",
    "        if not hasattr(df, 'write'):\n",
    "            raise ValueError(f\"Esperado DataFrame do Spark, recebido: {type(df)}\")\n",
    "        \n",
    "        if df.count() == 0:\n",
    "            logger.warning(\"DataFrame está vazio!\")\n",
    "        \n",
    "        valid_modes = [\"append\", \"overwrite\"]\n",
    "        if mode.lower() not in valid_modes:\n",
    "            raise ValueError(f\"Mode deve ser um entre as opções: {valid_modes}\")\n",
    "\n",
    "        valid_layers = [\"bronze\", \"silver\", \"gold\"]\n",
    "        if layer.lower() not in valid_layers:\n",
    "            raise ValueError(f\"Layer deve ser uma entre as opções: {valid_layers}\")\n",
    "\n",
    "        if not table_path or \".\" not in table_path:\n",
    "            raise ValueError(\"O table_path deve seguir formato 'catalog.schema.table'\")\n",
    "\n",
    "    def _create_table_if_not_exists(self):\n",
    "        \"\"\"Cria tabela se não existir\"\"\"\n",
    "        if not self.table_exists:\n",
    "            try:\n",
    "                spark.catalog.createTable(self.table_path, source=\"delta\", schema=self.df.schema)\n",
    "                logger.info(f\"✅ Tabela criada com sucesso: {self.table_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Erro ao criar a tabela: {self.table_path}\")\n",
    "\n",
    "    def _add_metadata(self):\n",
    "        \"\"\" Adiciona data_ingestao no DataFrame \"\"\"\n",
    "        if \"data_ingestao\" not in self.df.columns:\n",
    "            self.df = self.df.withColumn(\"data_ingestao\", F.current_date())\n",
    "\n",
    "    def _saveTable(self):\n",
    "        \"\"\" Salva a tabela de acordo com o mode informado \"\"\"\n",
    "        try:\n",
    "            if self.mode == \"append\":\n",
    "                self.df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(self.table_path)\n",
    "            elif self.mode == \"overwrite\":\n",
    "                self.df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(self.table_path)\n",
    "            logger.info(f\"✅ {self.df.count()} registros inseridos com sucesso!\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Erro ao salvar os dados na tabela {self.table_path}: {e}\")\n",
    "\n",
    "    def execute(self):\n",
    "        self._create_table_if_not_exists()\n",
    "        self._add_metadata()\n",
    "        self._saveTable()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5081829011939244,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "save_helper",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
