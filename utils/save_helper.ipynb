{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da9897b-4716-4eeb-b5f8-867159810d69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SaveHelper:\n",
    "    def __init__(self, df, layer, table_path, mode, key_columns=None):\n",
    "        self.df = df\n",
    "        self.layer = layer.lower()\n",
    "        self.table_path = table_path\n",
    "        self.table_exists = spark.catalog.tableExists(self.table_path)\n",
    "        self.mode = mode.lower()\n",
    "        self.key_columns = key_columns\n",
    "        \n",
    "        self._validate_inputs(df, layer, table_path, mode, key_columns)\n",
    "\n",
    "        logger.info(f\"SaveHelper inicializado - Tabela: {table_path} - Mode: {mode} - Layer: {layer}\")\n",
    "\n",
    "    def _validate_inputs(self, df, layer, table_path, mode, key_columns):\n",
    "        if df is None:\n",
    "            raise ValueError(\"DataFrame não pode ser None\")\n",
    "        \n",
    "        if not hasattr(df, 'write'):\n",
    "            raise ValueError(f\"Esperado DataFrame do Spark, recebido: {type(df)}\")\n",
    "        \n",
    "        if df.count() == 0:\n",
    "            logger.warning(\"DataFrame está vazio!\")\n",
    "        \n",
    "        valid_modes = [\"append\", \"overwrite\", \"delta\"]\n",
    "        if mode.lower() not in valid_modes:\n",
    "            raise ValueError(f\"Mode deve ser um entre as opções: {valid_modes}\")\n",
    "\n",
    "        valid_layers = [\"bronze\", \"silver\", \"gold\"]\n",
    "        if layer.lower() not in valid_layers:\n",
    "            raise ValueError(f\"Layer deve ser uma entre as opções: {valid_layers}\")\n",
    "\n",
    "        if not table_path or \".\" not in table_path:\n",
    "            raise ValueError(\"O table_path deve seguir formato 'catalog.schema.table'\")\n",
    "\n",
    "        if key_columns is not None and not isinstance(key_columns, list):\n",
    "            raise ValueError(f\"Esperado lista no parâmetro key_columns, recebido {type(key_columns)}\")\n",
    "        \n",
    "        if mode.lower() == \"delta\":\n",
    "            if key_columns is None or len(key_columns) == 0:\n",
    "                raise ValueError(\"Mode 'delta' requer key_columns para fazer o merge\")\n",
    "\n",
    "    def _create_table_if_not_exists(self):\n",
    "        if not self.table_exists:\n",
    "            try:\n",
    "                spark.catalog.createTable(self.table_path, source=\"delta\", schema=self.df.schema)\n",
    "                logger.info(f\"✅ Tabela criada com sucesso: {self.table_path}\")\n",
    "                self.table_exists = True\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Erro ao criar a tabela: {self.table_path} - {e}\")\n",
    "                raise\n",
    "\n",
    "    def _merge_data(self):\n",
    "        try:            \n",
    "            logger.info(f\"Executando merge com chaves: {self.key_columns}\")\n",
    "            \n",
    "            temp_view = \"temp_new_data_merge\"\n",
    "            self.df.createOrReplaceTempView(temp_view)\n",
    "            \n",
    "            join_conditions = []\n",
    "            for key in self.key_columns:\n",
    "                join_conditions.append(f\"target.{key} = source.{key}\")\n",
    "            \n",
    "            join_condition = \" AND \".join(join_conditions)\n",
    "            \n",
    "            columns = [col for col in self.df.columns if col != \"data_ingestao\"]\n",
    "            columns_str = \", \".join(columns)\n",
    "            values_str = \", \".join([f\"source.{col}\" for col in columns])\n",
    "            \n",
    "            if \"data_ingestao\" in self.df.columns:\n",
    "                columns_str += \", data_ingestao\"\n",
    "                values_str += \", source.data_ingestao\"\n",
    "            \n",
    "            merge_query = f\"\"\"\n",
    "                MERGE INTO {self.table_path} AS target\n",
    "                USING {temp_view} AS source\n",
    "                ON {join_condition}\n",
    "                \n",
    "                WHEN NOT MATCHED THEN\n",
    "                INSERT ({columns_str})\n",
    "                VALUES ({values_str})\n",
    "            \"\"\"\n",
    "            \n",
    "            logger.info(f\"Executando MERGE SQL...\")\n",
    "            spark.sql(merge_query)\n",
    "            logger.info(\"✅ Merge executado com sucesso!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Erro no merge: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _add_metadata(self):\n",
    "        if \"data_ingestao\" not in self.df.columns:\n",
    "            self.df = self.df.withColumn(\"data_ingestao\", F.current_date())\n",
    "            logger.info(\"✅ Coluna data_ingestao adicionada\")\n",
    "\n",
    "    def _saveTable(self):\n",
    "        try:\n",
    "            if self.mode == \"append\":\n",
    "                self.df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(self.table_path)\n",
    "                logger.info(f\"✅ {self.df.count()} registros inseridos com sucesso em modo APPEND!\")\n",
    "                \n",
    "            elif self.mode == \"overwrite\":\n",
    "                self.df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(self.table_path)\n",
    "                logger.info(f\"✅ Tabela sobrescrita com {self.df.count()} registros em modo OVERWRITE!\")\n",
    "                \n",
    "            elif self.mode == \"delta\":\n",
    "                self._merge_data()\n",
    "                logger.info(f\"✅ Merge realizado na tabela {self.table_path}!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Erro ao salvar os dados na tabela {self.table_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def execute(self):\n",
    "        logger.info(f\"Iniciando processo de salvamento - Mode: {self.mode}\")\n",
    "        \n",
    "        if self.layer == 'bronze':\n",
    "            self._add_metadata()\n",
    "        \n",
    "        if self.mode == \"delta\" and not self.table_exists:\n",
    "            logger.info(\"Tabela não existe para merge - criando tabela primeiro\")\n",
    "            self._create_table_if_not_exists()\n",
    "        \n",
    "        if self.mode != \"delta\":\n",
    "            self._create_table_if_not_exists()\n",
    "            \n",
    "        self._saveTable()\n",
    "        \n",
    "        logger.info(\"✅ Processo concluído com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5081829011939244,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "save_helper",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
